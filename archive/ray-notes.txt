problems to fix:
=========================================
fixed:
5. Leaked semaphore warnings
  UserWarning: resource_tracker: There appear to be 11 leaked semaphore objects to clean up at shutdown
  --- FIXED? (did cond.release() after proc.join())
=========================================
can't fix: ray's perturbation simply kills the bad-performing workers
7. The workers killed by perturbation result in this message:
  The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
  The last log is:
    (train_function pid=18843) -- 28.89s (18843) [PyConnector] INFO: Terminating VCMI PID=18852

  Which means my env attempts a graceful shutdown, but is killed in the process
  Logs from core-worker are:
  // the GRPC export metrics error is unrelated -- see timestamps
  [2023-11-20 13:11:17,853 W 18843 13264898] metric_exporter.cc:190: [1] Export metrics to agent failed: GrpcUnavailable: RPC Error message: failed to connect to all addresses; last error: UNKNOWN: ipv4:127. 0.0.1:63832: Failed to connect to remote host: Connection refused; RPC Error details: . This won't     affect Ray, but you can lose metrics from the cluster.
  [2023-11-20 13:11:36,490 I 18843 13264909] core_worker.cc:3826: Force kill actor request has received. exiting immediately... The actor is dead because all references to the actor were removed.
  [2023-11-20 13:11:36,490 W 18843 13264909] core_worker.cc:868: Force exit the process.  Details:       Worker exits because the actor is killed. The actor is dead because all references to the actor were   removed.
==========================================
fixed:
2. wandb STDOUT spam on each started run
    each ray step (eg. each rollout), ALL config values, incl. the const ones
==========================================
fixed:
1. graph log spam by ray in WANDB
==========================================
fixed:
  (caused by 4.) 6. Output directory already exists - main() tried so create out_dir every time
    File "/Users/simo/Projects/vcmi-gym/vcmi_gym/tools/common.py", line 111, in out_dir_from_template
      raise Exception("Output directory already exists: %s" % out_dir)
==========================================
fixed
3. gym warning on each started run
  UserWarning: WARN: Overriding environment local/VCMI-v0 already in registry.
==========================================
fixed
4. Started run ID is not recorded in config (as opposed to when using wandb-agent.py)
  => the utils/main.py generates and logs a new run_id
     ALSO: sometimes generated run_id is duplicated (not thread-safe apparently)
  This is bigger. wandb init in ray is not OK --
    - memleak for PB2 + WandbLoggerCallback: https://github.com/ray-project/ray/issues/40014
    - init via setup_wandb() is not working even with code from the tutorial
      (function thinks we are not get_world_rank==0 and exits early)
    - I may need to perform custom wandb init with custom sync values..?
  DONE, but now I get
    wandb: WARNING Tried to auto resume run with id a809e_00001 but id a809e_00000 is set.
  fixed, now I get:
    (tensorboard root_dir shit)
  fixed by DISABLING tensorboard in wandb: using wandb just for core metrics now
==========================================
idk, maybe resolved by closing the handlers. anyway, loglevel is now error
5. Increasingly duplicated python logs (but not prints):
  (train_function pid=26154) -- 94.10s (26154) [PyConnector] INFO: Terminating VCMI PID=26239
  (train_function pid=26154) -- 94.10s (26154) [PyConnector] INFO: Terminating VCMI PID=26239
  (train_function pid=26154) TEST CLOSED
  (train_function pid=26154) -- 94.10s (26154) [VcmiEnv] INFO: Env closed
  (train_function pid=26154) -- 94.10s (26154) [VcmiEnv] INFO: Env closed
==========================================
done
6. log changed hyperparameters to wandb
==========================================
filelocks of VCMI configs during VCMI init...
(maybe just lock a file in python pyconnector and other pyconnectors will wait)
