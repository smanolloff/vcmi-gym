---
program: wandb-agent.py
project: vcmi-gym
# method: grid
method: random
metric:
  name: rollout/ep_rew_mean
  goal: maximize
parameters:
  script:
    parameters:
      module: { value: "mppo_conv" }

  tags: { value: ["Classic", "Map-3stack-01"] }

  agent_load_file: { value: ~ }
  timesteps_total: { value: 100_000 }  # mandatory for sweep!
  timesteps_per_mapchange: { value: 0 }
  rollouts_total: { value: 0 }
  rollouts_per_mapchange: { value: 0 }
  rollouts_per_log: { value: 1 }
  rollouts_per_table_log: { value: 0 }
  success_rate_target: { value: 0 }
  ep_rew_mean_target: { value: 0 }
  quit_on_target: { value: true }
  mapside: { value: "attacker" }
  mapmask: { value: "gym/generated/88/88-3stack-01.vmap" }
  save_every: { value: 3600 } # seconds  # XXX: run sweeps with NO_SAVE=true
  permasave_every: { value: 7200 } # seconds  # XXX: run sweeps with NO_SAVE=true
  max_saves: { value: 3 }
  out_dir_template: { value: "data/{group_id}/{run_id}" }
  num_envs: { value: 1 }

  # TODO: use distributions for clearer visualization of sweeps
  # (to prevent line stacking and decoloring)
  # https://docs.wandb.ai/guides/sweeps/sweep-config-keys#distribution-options-for-random-and-bayesian-search

  # clip_coef: { value: 0.2 }
  clip_coef: { distribution: "uniform", min: 0.05, max: 0.5 }
  clip_vloss: { value: false }
  ent_coef: { value: 0.01 }
  gae_lambda: { distribution: "uniform", min: 0.5, max: 0.99 }
  gamma: { distribution: "uniform", min: 0.5, max: 0.99 }

  lr_schedule:
    parameters:
      mode: { value: "const" }
      start: { distribution: "inv_log_uniform_values", max: 1.0e-4, min: 1.0e-6 }

  max_grad_norm: { distribution: "uniform", min: 0.1, max: 10 }
  norm_adv: { value: true }
  num_minibatches: { value: 8 }
  # num_steps: { value: 64 }
  num_steps: { distribution: "q_uniform", q: 8, min: 16, max: 256 }
  stats_buffer_size: { value: 100 }
  update_epochs: { distribution: "int_uniform", min: 10, max: 50 }
  vf_coef: { distribution: "uniform", min: 0.1, max: 1.5 }
  weight_decay: { value: 0 }

  network:
    value:
      features_extractor:
        # => (B, 1, 11, 840)
        - {t: "Conv2d", in_channels: 1, out_channels: 16, kernel_size: [1, 56], stride: [1, 56], padding: 0}
        - {t: "LeakyReLU"}
        - {t: "BatchNorm2d", num_features: 16}
        # => (B, 16, 11, 15)
        - {t: "Flatten"}
        # => (B, 2640)
        - {t: "Linear", in_features: 2640, out_features: 64}
        - {t: "LeakyReLU"}
        # => (B, 64)
      actor: {t: "Linear", in_features: 64, out_features: 2311}
      critic: {t: "Linear", in_features: 64, out_features: 1}

  opponent_sbm_probs:
    value:
    - 1  # StupidAI
    - 0  # BattleAI
    - 0  # MMAI_MODEL

  env:
    parameters:
      reward_dmg_factor: { value: 5 }
      step_reward_fixed: { value: -100 }
      step_reward_mult: { value: 1 }
      term_reward_mult: { value: 0 }
      reward_clip_tanh_army_frac: { value: 1 }
      reward_army_value_ref: { value: 500 }
      random_combat: { value: 1 }
