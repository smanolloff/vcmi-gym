---
program: wandb-agent.py
project: vcmi-gym
# method: grid
method: random
metric:
  name: rollout/ep_rew_mean
  goal: maximize
parameters:
  script:
    parameters:
      module: { value: "mppo_heads" }

  group_id: { value: "sweeps" }
  agent_load_file: { value: ~ } #"/Users/simo/Projects/vcmi-gym/data/A3-sweep/h98k5nhr/agent-1710918951.zip"}
  timesteps_total: { value: 50_000 }
  timesteps_per_mapchange: { value: 0 }
  rollouts_total: { value: 0 }
  rollouts_per_mapchange: { value: 0 }
  rollouts_per_log: { value: 1 }
  rollouts_per_table_log: { value: 0 }
  success_rate_target: { value: 0 }
  ep_rew_mean_target: { value: 0 }
  quit_on_target: { value: true }
  mapside: { value: "attacker" }
  mapmask: { value: "gym/generated/88/88-3stack-01.vmap" }
  save_every: { value: 3600 } # seconds
  max_saves: { value: 3 }
  out_dir_template: { value: "data/{group_id}/{run_id}" }
  num_envs: { value: 1 }

  # TODO: use distributions for clearer visualization of sweeps
  # (to prevent line stacking and decoloring)
  # https://docs.wandb.ai/guides/sweeps/sweep-config-keys#distribution-options-for-random-and-bayesian-search

  # clip_coef: { value: 0.2 }
  clip_coef: { distribution: "uniform", min: 0.05, max: 0.5 }
  clip_vloss: { value: true }
  ent_coef: { value: 0.007 }
  gae_lambda: { distribution: "uniform", min: 0.7, max: 0.99 }
  gamma: { value: 0.8 }
  # learning_rate: { values: [0.0001, 0.0003, 0.0005] }
  learning_rate: { distribution: "inv_log_uniform_values", min: 0.000001, max: 0.0001 }
  max_grad_norm: { value: 1 }
  norm_adv: { value: true }
  num_minibatches: { value: 8 }
  # num_steps: { value: 64 }
  num_steps: { distribution: "q_uniform", q: 8, min: 16, max: 256 }
  stats_buffer_size: { value: 100 }
  update_epochs: { distribution: "int_uniform", min: 10, max: 250 }
  vf_coef: { value: 0.3 }
  weight_decay: { value: 0 }

  loss_weights:
    value:
      DEFEND: [1, 0, 0]
      WAIT: [1, 0, 0]
      SHOOT: [0.5, 0.5, 0]
      MOVE: [0.5, 0.5, 0]
      AMOVE: [0.33, 0.33, 0.34]

  opponent_sbm_probs:
    value:
    - 1  # StupidAI
    - 0  # BattleAI
    - 0  # MMAI_MODEL

  env:
    parameters:
      reward_dmg_factor: { value: 5 }
      step_reward_fixed: { value: -100 }
      step_reward_mult: { value: 1 }
      term_reward_mult: { value: 0 }
      reward_clip_tanh_army_frac: { value: 1 }
      reward_army_value_ref: { value: 500 }
      random_combat: { value: true }
