---
group_id: qrdqn-from-h3qh72hg
run_name: "Q=100 T=4 U=10K lr=1e-7 buf=5K"
tags: ["Map-3stack-01", "StupidAI", "OneHot"]

wandb_project: vcmi-gym
run_id: ~
resume: False
overwrite: []
# overwrite: ["learning_rate", "weight_decay"]

notes: ""

# agent_load_file: ~
agent_load_file: "data/qrdqn-from-h3qh72hg/80qqlepn/agent-nobuf-1712755268.pt"
vsteps_total: 0  # exp_decay requires vsteps_total
vsteps_for_warmup: 10000
random_warmup: false
trains_per_log: 25
opponent_load_file: ~
success_rate_target: ~
ep_rew_mean_target: ~
quit_on_target: false
mapside: "attacker"  # attacker/defender/both
mapmask: "gym/generated/88/88-3stack-300K-01.vmap"
randomize_maps: false
save_every: 10800  # seconds
permasave_every: 999999999  # seconds
max_saves: 2
out_dir_template: "data/{group_id}/{run_id}"

# SBM = StupidAI, BattleAI, MMAI_MODEL
opponent_sbm_probs: [1, 0, 0]
opponent_load_file: ~

# XXX: 21 envs require 256+ filehandles (and 256 is the limit by default)
#      To increase (current shell session only):
#           ulimit -n 1024
num_envs: 1

n_quantiles: 100
network:
  # => (B, 11, 15, 86|574)
  - {t: "Flatten", start_dim: 2}
  - {t: "Unflatten", dim: 1, unflattened_size: [1, 11]}
  # => (B, 1, 11, 1290|8610)
  - {t: "Conv2d", in_channels: 1, out_channels: 32, kernel_size: [1, 86], stride: [1, 86], padding: 0}
  # - {t: "Conv2d", in_channels: 1, out_channels: 32, kernel_size: [1, 574], stride: [1, 574], padding: 0}
  - {t: "LeakyReLU"}
  - {t: "Flatten"}
  # => (B, 5280)
  - {t: "Linear", in_features: 5280, out_features: 1024}
  - {t: "LeakyReLU"}
  # => (B, 64)


#
# QRDQN Hyperparams
#

# lr_schedule: { mode: "lin_decay", start: 1.0e-5, end: 1.0e-6, rate: 4 }
lr_schedule: { mode: "const", start: 1.0e-7, end: 1.0e-7, rate: 4 }
eps_schedule: { mode: "const", start: 0.1, end: 0.01, rate: 4 }
buffer_size_vsteps: 5000
batch_size: 32
tau: 1.0
vsteps_per_train: 4
train_iterations: 1
trains_per_target_update: 10000
gamma: 0.75
max_grad_norm: 0.5

env:
  encoding_type: "float"
  reward_dmg_factor: 5
  step_reward_fixed: -100
  step_reward_mult: 1
  term_reward_mult: 0
  reward_clip_tanh_army_frac: 1
  reward_army_value_ref: 500
  random_combat: 1

env_wrappers: []
logparams: {}
