---
group_id: heads-vs-classic
run_name: "[H] as yrzdlvel, lr=5e-7"
tags: ["Heads", "Map-3stack-01"]

wandb_project: vcmi-gym
run_id: ~
resume: False
overwrite: []
# overwrite: ["learning_rate", "weight_decay"]

notes: ""

agent_load_file: ~
# agent_load_file: "data/sweeps/ujw53ibc/agent-1711299495.zip"
rollouts_total: 0
timesteps_total: 0  # exp_decay requires timesteps_total
timesteps_per_mapchange: 0
rollouts_per_mapchange: 0
rollouts_per_log: 1
rollouts_per_table_log: 0
opponent_load_file: ~
success_rate_target: ~
ep_rew_mean_target: ~
quit_on_target: false
mapside: "attacker"  # attacker/defender/both
mapmask: "gym/generated/88/88-3stack-01.vmap"
randomize_maps: false
save_every: 3600  # seconds
max_saves: 3
out_dir_template: "data/{group_id}/{run_id}"

# SBM = StupidAI, BattleAI, MMAI_MODEL
opponent_sbm_probs: [1, 0, 0]
opponent_load_file: ~

# XXX: 21 envs require 256+ filehandles (and 256 is the limit by default)
#      To increase (current shell session only):
#           ulimit -n 1024
num_envs: 1


#
# PPO Hyperparams
#

clip_coef: 0.19
clip_vloss: false
ent_coef: 0.007
gae_lambda: 0.7
gamma: 0.8
lr_schedule: { mode: const, start: 5.0e-7 }
max_grad_norm: 1
norm_adv: true
num_minibatches: 8      # minibatch_size = rollout_buffer/num_minibatches
num_steps: 64           # rollout_buffer = num_steps*num_envs
stats_buffer_size: 100  # ~= episodes per rollout
update_epochs: 50       # full passes of rollout_buffer
vf_coef: 0.3
weight_decay: 0

loss_weights:
  DEFEND: [1, 0, 0]
  WAIT: [1, 0, 0]
  SHOOT: [1, 1, 0]
  MOVE: [1, 1, 0]
  AMOVE: [1, 1, 1]

env:
  reward_dmg_factor: 5
  step_reward_fixed: -100
  step_reward_mult: 1
  term_reward_mult: 0
  reward_clip_tanh_army_frac: 1
  reward_army_value_ref: 1000
  random_combat: true

env_wrappers: []
logparams: {}
