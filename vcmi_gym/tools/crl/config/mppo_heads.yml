---
group_id: heads
run_id: heads-A1-6
resume: False
overwrite: []
# overwrite: ["learning_rate", "weight_decay"]

notes: ""

agent_load_file: ~
# agent_load_file: "data/crl/crl-mult-sanity2-lr0.0001-wd0/agent-1709801900.pt"
rollouts_total: 0
rollouts_per_mapchange: 0
rollouts_per_log: 1
rollouts_per_table_log: 10
opponent_load_file: ~
success_rate_target: ~
mapmask: "gym/A1.vmap"
randomize_maps: false
save_every: 3600  # seconds
max_saves: 3
out_dir_template: "data/{group_id}/{run_id}"

# SBM = StupidAI, BattleAI, MMAI_MODEL
opponent_sbm_probs: [1, 0, 0]
opponent_load_file: ~

# XXX: 21 envs require 256+ filehandles (and 256 is the limit by default)
#      To increase (current shell session only):
#           ulimit -n 1024
num_envs: 1

#
# PPO Hyperparams
#
weight_decay: 0.05
learning_rate: 0.00003
num_steps: 256       # rollout_buffer = num_steps*num_envs
num_minibatches: 16  # minibatch_size = rollout_buffer/num_minibatches
update_epochs: 10    # full passes of rollout_buffer
gamma: 0.8
gae_lambda: 0.8
norm_adv: true
clip_coef: 0.3
clip_vloss: true
ent_coef: 0.1
vf_coef: 1.2
max_grad_norm: 0.5
target_kl: ~

loss_weights:
  DEFEND: [1, 0, 0]
  WAIT: [1, 0, 0]
  SHOOT: [0.5, 0.5, 0]
  MOVE: [0.5, 0.5, 0]
  AMOVE: [0.33, 0.33, 0.34]

env:
  max_steps: 500
  reward_dmg_factor: 5
  vcmi_loglevel_global: "error"
  vcmi_loglevel_ai: "error"
  vcmienv_loglevel: "WARN"
  consecutive_error_reward_factor: -1
  sparse_info: true
  step_reward_mult: 1
  term_reward_mult: 0
  reward_clip_mod: ~

  # 0=hex.id, 1=hex.state, 2=qty, 12=qpos, 13=side, 15=creature type
  #   Visualize with:
  #   o = obs.reshape(11, 15, 6)[0][0] * [164, 4, 5001, 15, 2, 151])
  #   o.astype(int)
  # hexattr_filter: [0, 1, 2, 12, 13, 15]

  # Set dynamically
  # mapname: "ai/generated/A01.vmap"
  # attacker: "MMAI_USER"  # MMAI_USER / MMAI_MODEL / StupidAI / BattleAI
  # defender: "StupidAI"   # MMAI_USER / MMAI_MODEL / StupidAI / BattleAI
  # attacker_model: ~  # MPPO zip model (if attacker=MMAI_MODEL)
  # defender_model: ~  # MPPO zip model (if defender=MMAI_MODEL)

env_wrappers: []

logparams:
  "config/rollouts_per_mapchange": "rollouts_per_mapchange"
  "config/learning_rate": "learning_rate"
  "config/gamma": "gamma"
  "config/ent_coef": "ent_coef"
  "config/vf_coef": "vf_coef"
  "config/max_grad_norm": "max_grad_norm"
  "config/weight_decay": "weight_decay"
  "config/num_steps": "num_steps"
  "config/num_minibatches": "num_minibatches"
  "config/update_epochs": "update_epochs"
  "config/gae_lambda": "gae_lambda"
  "config/norm_adv": "norm_adv"
  "config/clip_coef": "clip_coef"
  "config/clip_vloss": "clip_vloss"
  "config/env/reward_dmg_factor": "env.reward_dmg_factor"
  "config/env/max_steps": "env.max_steps"
  "config/env/CERF": "env.consecutive_error_reward_factor"
  "config/env/step_reward_mult:": "env.step_reward_mult"
  "config/env/term_reward_mult:": "env.term_reward_mult"
  "config/env/reward_clip_mod:": "env.reward_clip_mod"
