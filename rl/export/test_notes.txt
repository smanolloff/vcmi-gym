1. Use this debug return in ExecuTorchModel's predict method
    (in rl/algos/mppo_dna_heads/mppo_dna_heads_new.py):

        return (
            action_logits,                     # [0]
            hex1_logits,                       # [1]
            hex2_logits,                       # [2]
            probs_act0,                        # [3]
            probs_hex1,                        # [4]
            probs_hex2,                        # [5]
            mask_action,                       # [6]
            mask_hex1[0][act0],                # [7]
            mask_hex2[0, act0, hex1],          # [8]
            act0,                              # [9]
            hex1,                              # [10]
            hex2,                              # [11]
            action,                            # [12]
            obs[:, self.global_wait_offset],   # [13]
            mask_action[:, 0]                  # [14]
        )


2. Export the model (set MODEL_EXPORT_PATH inside):

    python -m rl.export.export

3. Place these functions in in vcmi/AI/MMAI/BAI/model/ExecuTorchModel.cpp
    and invoke debug_model() as the first statement in the constructor body:

        std::vector<float> read_f32_bin(const std::string& path) {
            std::cout << "reading from file: " << path << "\n";
            std::ifstream f(path, std::ios::binary);
            if (!f) throw std::runtime_error("failed to open file");

            f.seekg(0, std::ios::end);
            std::streampos end = f.tellg();
            if (end < 0) throw std::runtime_error("tellg failed");
            const size_t nbytes = static_cast<size_t>(end);
            if (nbytes % sizeof(float) != 0)
                throw std::runtime_error("file size not a multiple of sizeof(float)");

            std::vector<float> data(nbytes / sizeof(float));
            f.seekg(0, std::ios::beg);
            f.read(reinterpret_cast<char*>(data.data()), nbytes);
            if (!f) throw std::runtime_error("binary read failed");
            return data;
        }

        void print_shape(const executorch::runtime::etensor::Tensor* t) {
            const auto& s = t->sizes();
            std::cout << "shape=[";
            for (size_t i = 0; i < s.size(); ++i) {
                if (i) std::cout << ", ";
                std::cout << s[i];
            }
            std::cout << "] numel=" << t->numel() << "\n";
        }

        void print_tensor_f32(const executorch::runtime::etensor::Tensor* t) {
            // print_shape(t);
            const float* p = t->const_data_ptr<float>();
            size_t n = static_cast<size_t>(t->numel());
            std::cout << "[";
            for (size_t i = 0; i < n-1; ++i) std::cout << p[i] << ",";
            std::cout << p[n-1] << "]";
        }

        void print_tensor_bool(const executorch::runtime::etensor::Tensor* t) {
            // print_shape(t);
            const bool* p = t->const_data_ptr<bool>();
            size_t n = static_cast<size_t>(t->numel());
            std::cout << "[";
            for (size_t i = 0; i < n-1; ++i) std::cout << p[i] << ",";
            std::cout << p[n-1] << "]";
        }

        void print_tensor_long(const executorch::runtime::etensor::Tensor* t) {
            // print_shape(t);
            const uint64_t* p = t->const_data_ptr<uint64_t>();
            size_t n = static_cast<size_t>(t->numel());
            std::cout << "[";
            for (size_t i = 0; i < n-1; ++i) std::cout << p[i] << ",";
            std::cout << p[n-1] << "]";
        }

        void debug_model() {
            auto obspath = std::string("");
            if (const char* v = std::getenv("OBS_PATH")) obspath = std::string(v);
            if (obspath.empty())
                throw std::runtime_error("Env var \"OBS_PATH\" is required");

            auto modelpath = std::string("");
            if (const char* v = std::getenv("MODEL_PATH")) modelpath = std::string(v);
            if (modelpath.empty())
                throw std::runtime_error("Env var \"MODEL_PATH\" is required");

            auto buf = read_f32_bin(obspath);
            auto module = executorch::extension::Module(modelpath);
            auto tensor = executorch::extension::from_blob(buf.data(), {28114});
            auto res = module.execute("predict", tensor);

            std::cout << "{\n";
            std::cout << "  \"model_path\": \"" << modelpath << "\",\n";
            std::cout << "  \"obs_path\": \"" << obspath << "\",\n";
            std::cout << "  \"results\": [\n";

            auto ev0 = res->at(0);
            assert(ev0.isTensor());
            auto t0 = ev0.toTensor();
            assert(t0.dtype() == executorch::aten::ScalarType::Float);
            std::cout << "    {\n";
            std::cout << "      \"name\": \"action_logits\",\n";
            std::cout << "      \"data\": ";
            print_tensor_f32(&t0);
            std::cout << "\n    },\n";

            auto ev1 = res->at(1);
            assert(ev1.isTensor());
            auto t1 = ev1.toTensor();
            assert(t1.dtype() == executorch::aten::ScalarType::Float);
            std::cout << "    {\n";
            std::cout << "      \"name\": \"hex1_logits\",\n";
            std::cout << "      \"data\": ";
            print_tensor_f32(&t1);
            std::cout << "\n    },\n";

            auto ev2 = res->at(2);
            assert(ev2.isTensor());
            auto t2 = ev2.toTensor();
            assert(t2.dtype() == executorch::aten::ScalarType::Float);
            std::cout << "    {\n";
            std::cout << "      \"name\": \"hex2_logits\",\n";
            std::cout << "      \"data\": ";
            print_tensor_f32(&t2);
            std::cout << "\n    },\n";

            auto ev3 = res->at(3);
            assert(ev3.isTensor());
            auto t3 = ev3.toTensor();
            assert(t3.dtype() == executorch::aten::ScalarType::Float);
            std::cout << "    {\n";
            std::cout << "      \"name\": \"probs_act0\",\n";
            std::cout << "      \"data\": ";
            print_tensor_f32(&t3);
            std::cout << "\n    },\n";

            auto ev4 = res->at(4);
            assert(ev4.isTensor());
            auto t4 = ev4.toTensor();
            assert(t4.dtype() == executorch::aten::ScalarType::Float);
            std::cout << "    {\n";
            std::cout << "      \"name\": \"probs_hex1\",\n";
            std::cout << "      \"data\": ";
            print_tensor_f32(&t4);
            std::cout << "\n    },\n";

            auto ev5 = res->at(5);
            assert(ev5.isTensor());
            auto t5 = ev5.toTensor();
            assert(t5.dtype() == executorch::aten::ScalarType::Float);
            std::cout << "    {\n";
            std::cout << "      \"name\": \"probs_hex2\",\n";
            std::cout << "      \"data\": ";
            print_tensor_f32(&t5);
            std::cout << "\n    },\n";

            auto ev6 = res->at(6);
            assert(ev6.isTensor());
            auto t6 = ev6.toTensor();
            assert(t6.dtype() == executorch::aten::ScalarType::Bool);
            std::cout << "    {\n";
            std::cout << "      \"name\": \"mask_action\",\n";
            std::cout << "      \"data\": ";
            print_tensor_bool(&t6);
            std::cout << "\n    },\n";

            auto ev7 = res->at(7);
            assert(ev7.isTensor());
            auto t7 = ev7.toTensor();
            assert(t7.dtype() == executorch::aten::ScalarType::Bool);
            std::cout << "    {\n";
            std::cout << "      \"name\": \"mask_hex1\",\n";
            std::cout << "      \"data\": ";
            print_tensor_bool(&t7);
            std::cout << "\n    },\n";

            auto ev8 = res->at(8);
            assert(ev8.isTensor());
            auto t8 = ev8.toTensor();
            assert(t8.dtype() == executorch::aten::ScalarType::Bool);
            std::cout << "    {\n";
            std::cout << "      \"name\": \"mask_hex2\",\n";
            std::cout << "      \"data\": ";
            print_tensor_bool(&t8);
            std::cout << "\n    },\n";

            auto ev9 = res->at(9);
            assert(ev9.isTensor());
            auto t9 = ev9.toTensor();
            assert(t9.dtype() == executorch::aten::ScalarType::Long);
            std::cout << "    {\n";
            std::cout << "      \"name\": \"act0\",\n";
            std::cout << "      \"data\": ";
            print_tensor_long(&t9);
            std::cout << "\n    },\n";

            auto ev10 = res->at(10);
            assert(ev10.isTensor());
            auto t10 = ev10.toTensor();
            assert(t10.dtype() == executorch::aten::ScalarType::Long);
            std::cout << "    {\n";
            std::cout << "      \"name\": \"hex1\",\n";
            std::cout << "      \"data\": ";
            print_tensor_long(&t10);
            std::cout << "\n    },\n";

            auto ev11 = res->at(11);
            assert(ev11.isTensor());
            auto t11 = ev11.toTensor();
            assert(t11.dtype() == executorch::aten::ScalarType::Long);
            std::cout << "    {\n";
            std::cout << "      \"name\": \"hex2\",\n";
            std::cout << "      \"data\": ";
            print_tensor_long(&t11);
            std::cout << "\n    },\n";

            auto ev12 = res->at(12);
            assert(ev12.isTensor());
            auto t12 = ev12.toTensor();
            assert(t12.dtype() == executorch::aten::ScalarType::Long);
            std::cout << "    {\n";
            std::cout << "      \"name\": \"action\",\n";
            std::cout << "      \"data\": ";
            print_tensor_long(&t12);
            std::cout << "\n    },\n";

            auto ev13 = res->at(13);
            assert(ev13.isTensor());
            auto t13 = ev13.toTensor();
            assert(t13.dtype() == executorch::aten::ScalarType::Float);
            std::cout << "    {\n";
            std::cout << "      \"name\": \"obs[:, 14]\",\n";
            std::cout << "      \"data\": ";
            print_tensor_f32(&t13);
            std::cout << "\n    },\n";

            auto ev14 = res->at(14);
            assert(ev14.isTensor());
            auto t14 = ev14.toTensor();
            assert(t14.dtype() == executorch::aten::ScalarType::Bool);
            std::cout << "    {\n";
            std::cout << "      \"name\": \"mask[:, 0]\",\n";
            std::cout << "      \"data\": ";
            print_tensor_bool(&t14);
            std::cout << "\n    }\n";
            std::cout << "  ]\n";
            std::cout << "}\n";

            throw std::runtime_error("DEBUG: forced quit");
       }

5. Recompile VCMI in PLAY mode (use the `mmai` branch, not `mmai-ml` branch) and start it with env vars:

    MODEL_PATH=$PWD/Mods/MMAI/models/cgfpenwh-1756282288.pte OBS_PATH=$VCMIGYM/rl/export/obs.np rel/bin/vcmilauncher

6. In launcher UI, set Neutral AI to MMAI (experimental) and launch the game
7. Start a battle. It will print a debug JSON to STDOUT on battle start
8. Take the debug output, copy it to cppout.json
9. Compare model outputs (make sure to adjust MODEL_PATH)

    python -m rl.export.compare

10. If behaviour is weird (e.g. very different outputs), try isolating the problem with dummy.py

To generate a new obs.np, use env.obs["observation"].tofile("...")
