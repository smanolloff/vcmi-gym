Issues in 2.38.0:

* moderate:
  ray creates 2 local environments even if there runners > 0.
  This prevents conntype=thread as the local runners are in the main proc.
  Workaround: just create a DummyEnv if workers>0 && worker_id==0

* major:
  evaluation with N episodes continues for N more episodes
  Opened PR: https://github.com/ray-project/ray/pull/48499

* moderate:
  tune calls trainable.step() once per perturbation
  then calls trainable.log_result() ->
    -> which calls MPPOCallback.on_train_result() to return tune metrics

  Eval results are used by tune
  => on_train_result MUST have eval results
  => algo config.evaluation_interval MUST BE 1

  However, this means wandb metrics will be logged very rarely (just on perturb)
  ... on_train_result() is also the ONLY useful callback for wandb logging
  => Workaround:
    1. Override Algorithm.step() to actually step N times
      - call super() N times and store/aggregate specific metrics:
        - TIME_THIS_ITER_S (summed accross the N sub-iterations)
        - TIMESTEPS_THIS_ITER (summed) - if available
        - EPISODES_THIS_ITER (summed) - if available
        - any user-defined metrics (eval/net_value, etc.)
      - return (only) the aggregated metrics

* minor:
  PPO adds an extra terminal observation which means batch sizes increase
  proportionally to the number of episodes.
  E.g. 512 steps with avg. 50 terminations would result in 562 steps.
  The "terminal" observations are passed through the NN but the "critic"
  value is unused in GAE calculations, and their "actor" logits are masked
  in KL loss calculations. I checked and they have no effect on the loss calc.
  Effect on me: useless samples are fed through the NN

* trivial:
  Different distributions are used for KL loss and surrogate loss calculations.
  Left a PR here: https://github.com/ray-project/ray/pull/47889/files#r1814477847
  Effect on me: none (I use the same distributions)
