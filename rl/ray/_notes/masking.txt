
# My CategoricalMasked included a monkey-patch of the `entropy()` method
# to ensure masked actions (which still have non-0 probs) do not affect
# the entropy calculation.
# However, non-0 probs means it's also possible to sample such an action,
# but there was never a patch for `sample()` method.
#
# ray's Masked example does not even account for those non-0 probs and relies
# on the default Categorical distribution. Maybe it just makes no sense to have
# special handling at all.
#
# => just use plain Categorical distribution
#
# class CategoricalMasked(torch.distributions.Categorical):
#     def entropy(self):
#         # Highly negative logits don't result in 0 probs, so we must replace
#         # with 0s to ensure 0 contribution to the distribution's entropy
#         p_log_p = self.logits * self.probs
#         p_log_p = torch.where(self.logits > FLOAT_MIN, p_log_p, torch.tensor(0, dtype=p_log_p.dtype, device=p_log_p.device))
#         return -p_log_p.sum(-1)
