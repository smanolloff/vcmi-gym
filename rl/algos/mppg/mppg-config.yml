---
group_id: mppg
run_name: "mppg post-refactor"
tags: ["Map-3stack-01", "StupidAI", "encoding-float", "MPPG"]

wandb_project: vcmi-gym
run_id: ~
resume: False
overwrite: []
# overwrite: ["learning_rate", "weight_decay"]

notes: ""

agent_load_file: ~
#agent_load_file: "data/sweeps/vou0cebq/agent-1712277867.pt"
vsteps_total: 0  # exp_decay requires vsteps_total or seconds_total
seconds_total: 0  # mutually-exclusive with vsteps_total
rollouts_per_mapchange: 0
rollouts_per_log: 1
rollouts_per_table_log: 0
opponent_load_file: ~
success_rate_target: ~
ep_rew_mean_target: ~
quit_on_target: false
mapside: "attacker"  # attacker/defender/both
mapmask: "gym/generated/88/88-3stack-30K-01.vmap"
randomize_maps: false
save_every: 3600  # seconds
permasave_every: 10800  # seconds
max_saves: 3
out_dir_template: "data/{group_id}/{run_id}"  # XXX: relative to cwd()

# SBM = StupidAI, BattleAI, MMAI_MODEL
opponent_sbm_probs: [1, 0, 0]
opponent_load_file: ~

num_envs: 1

network:
  features_extractor:
    # => (B, 11, 15, 86|574)
    - {t: "Flatten", start_dim: 2}
    - {t: "Unflatten", dim: 1, unflattened_size: [1, 11]}
    # => (B, 1, 11, 1290|8610)
    - {t: "Conv2d", in_channels: 1, out_channels: 32, kernel_size: [1, 86], stride: [1, 86], padding: 0}
    - {t: "LeakyReLU"}
    # => (B, 32, 11, 15)
    - {t: "Conv2d", in_channels: 32, out_channels: 64, kernel_size: 1, stride: 1, padding: 0}
    - {t: "LeakyReLU"}
    # => (B, 64, 11, 15)
    - {t: "LeakyReLU"}
    - {t: "Flatten"}
    # => (B, 10560)
    - {t: "Linear", in_features: 10560, out_features: 1024}
    - {t: "LeakyReLU"}
    # => (B, 1024)
    - {t: "Linear", in_features: 1024, out_features: 1024}
    - {t: "LeakyReLU"}
    # => (B, 1024)

  actor: {t: "Linear", in_features: 1024, out_features: 2311}
  critic: {t: "Linear", in_features: 1024, out_features: 1}

#
# PPG Hyperparams
#

clip_coef: 0.4
clip_vloss: false
ent_coef: 0.007
gae_lambda: 0.8
gamma: 0.8425
lr_schedule: { mode: const, start: 0.00001, end: 1.0e-6, rate: 10 }
max_grad_norm: 0.5
norm_adv: "batch"
num_minibatches: 2   # minibatch_size = rollout_buffer/num_minibatches
num_steps: 128       # rollout_buffer = num_steps*num_envs
e_policy: 10         # this is PPO's "update_epochs"
vf_coef: 1.2
weight_decay: 0

rollouts_per_phase: 32      # orig name is "n_iteration"
e_auxiliary: 6              # think PPG's "aux_update_epochs"
beta_clone: 1.0
num_aux_rollouts: 4
n_aux_grad_accum: 1

env:
  encoding_type: "float"
  reward_dmg_factor: 5
  step_reward_fixed: -100
  step_reward_mult: 1
  term_reward_mult: 0
  reward_clip_tanh_army_frac: 1
  reward_army_value_ref: 500
  random_combat: 1

env_wrappers: []
logparams: {}
