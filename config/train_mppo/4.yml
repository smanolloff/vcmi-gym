---
seed: ~
run_id: l3-do-bn-fdim1024-rollouts100-lr0.00126-e0
group_id: fe-arch
# model_load_file: __latest__
model_load_file: "data/fe-arch/l3-do-bn-fdim1024-rollouts100-lr0.0001-e0-1706598388/model.zip"
# features_extractor_load_file: "autoencoder-pretrained-encoder-params.pth"
features_extractor_load_file: ~
features_extractor_load_file_type: "params"  # model / params / sb3
features_extractor_freeze: false
notes: ""
out_dir_template: "data/{group_id}/{run_id}"
observations_dir: ~
log_tensorboard: true
progress_bar: false  # too annoying and noisy
# total_timesteps: !!float 100000
rollouts_total: 0
rollouts_per_iteration: 100
rollouts_per_log: 2
save_every: 7200  # seconds
max_saves: 3
iteration: 140  # start iteration (to skip first n_envs/2 maps)

# XXX: 21 envs require 256+ filehandles (and 256 is the limit by default)
#      To increase (current shell session only):
#           ulimit -n 1024
n_envs: 8
framestack: 1

# overwrites learner_kwargs.n_steps to n_global_steps_max // n_envs
# (eg. 2048/48 = 42.666... => n_steps=42)
n_global_steps_max: 1024

mapmask: "ai/generated/B*.vmap"
randomize_maps: false

logparams: {}
  # "config/weight_decay": "learner_kwargs.policy_kwargs.optimizer_kwargs.weight_decay"
  # "config/features_extractor": "learner_kwargs.policy_kwargs.features_extractor_class_name"
  # "config/optimizer": "learner_kwargs.policy_kwargs.optimizer_class_name"
  # "config/rew_clip": "env_kwargs.reward_clip_mod"

activation: "LeakyReLU"
net_arch: []
optimizer:
  class_name: "AdamW"
  kwargs: {eps: !!float 1e-5, weight_decay: 0}
features_extractor:
  class_name: "VcmiFeaturesExtractor"
  kwargs:
    layers:
        # => (11, 225)
        - {t: "Conv2d", out_channels: 32, kernel_size: [1, 15], stride: [1, 15], padding: 0}
        - {t: "BatchNorm2d", num_features: 32}
        - {t: "LeakyReLU"}
        - {t: "Dropout2d", p: 0.25, inplace: true}
        # => (11, 15)
        - {t: "Conv2d", in_channels: 32, out_channels: 64, kernel_size: 3, stride: 2, padding: 0}
        - {t: "BatchNorm2d", num_features: 64}
        - {t: "LeakyReLU"}
        - {t: "Dropout2d", p: 0.25, inplace: true}
        # => (7, 5)
        - {t: "Conv2d", in_channels: 64, out_channels: 64, kernel_size: 3, stride: 2, padding: 0}
        - {t: "BatchNorm2d", num_features: 64}
        - {t: "LeakyReLU"}
        - {t: "Dropout2d", p: 0.25, inplace: true}
        # => (3, 2)
        - {t: "Flatten"}
        # => 384
        # TODO: remove
        - {t: "Linear", in_features: 384, out_features: 1024}
        - {t: "LeakyReLU"}
        # => 1024

learner_kwargs:
  # n_steps: 512  # calculated dynamically (see n_global_steps_max)
  batch_size: 64
  n_epochs: 10
  gamma: 0.8425
  gae_lambda: 0.8
  clip_range: 0.4
  normalize_advantage: true
  ent_coef: 0
  vf_coef: 0.6
  max_grad_norm: 2.5

# Examples:
#   * "const_0.001"
#   * "lin_decay_0.03_0.0001_0.75"
#   * "exp_decay_0.03_0.0001_0.5_5"
learner_lr_schedule: "const_0.00126"

env_kwargs:
  max_steps: 500
  reward_clip_mod: ~
  reward_dmg_factor: 0
  vcmi_loglevel_global: "error"
  vcmi_loglevel_ai: "error"
  vcmienv_loglevel: "WARN"
  consecutive_error_reward_factor: -1
  sparse_info: true

  # Set dynamically
  # mapname: "ai/generated/A01.vmap"
  # attacker: "MMAI_USER"  # MMAI_USER / MMAI_MODEL / StupidAI / BattleAI
  # defender: "StupidAI"   # MMAI_USER / MMAI_MODEL / StupidAI / BattleAI
  # attacker_model: ~  # MPPO zip model (if attacker=MMAI_MODEL)
  # defender_model: ~  # MPPO zip model (if defender=MMAI_MODEL)

env_wrappers: []

